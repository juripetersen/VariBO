FROM maven:3.9.4-eclipse-temurin-11-focal

USER root

ARG CUDA_VERSION=12.6.1
ARG CUDNN_VERSION=9.5.0.50
ARG OS=ubuntu22.04

RUN apt update && \
    apt upgrade -y && \
    apt-get install -y software-properties-common && \
    add-apt-repository ppa:deadsnakes/ppa -y && \
    apt update && \
    apt-get install -y \
    g++ \
    curl \
    ca-certificates \
    gcc \
    make \
    cmake \
    wget \
    gnupg \
    libsnappy-dev \
    libzip-dev \
    openssl \
    python3.11-full \
    python3.11-dev \
    python3-pip \
    unzip

# Add NVIDIA CUDA repo
RUN apt-get update && \
    apt-get install -y wget gnupg && \
    wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-keyring_1.1-1_all.deb && \
    dpkg -i cuda-keyring_1.1-1_all.deb && \
    apt-get update

# Install CUDA 12.4 toolkit and cuDNN
RUN apt-get install -y \
    cuda-toolkit-12-4 \
    libcudnn9-cuda-12 \
    libcudnn9-dev-cuda-12

#RUN wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-keyring_1.0-1_all.deb && \
#dpkg -i cuda-keyring_1.0-1_all.deb && \
#apt-get -y update && \
#apt-get -y install cuda-toolkit && \
#DEBIAN_FRONTEND=noninteractive apt-get -y install nvidia-gds nvidia-driver-580 && \
#apt-get -y install libcudnn9-cuda-13 libcudnn9-dev-cuda-13 libcudnn9-samples

#RUN export PATH="/usr/local/cuda/bin:$PATH" && \
#    export LD_LIBRARY_PATH="/usr/local/cuda/lib64:$LD_LIBRARY_PATH"

## legacy (manual cudnn installation)
#RUN CUDNN_TAR_FILE="cudnn-linux-x86_64-8.7.0.84_cuda11-archive.tar.xz" && \
# wget https://developer.download.nvidia.com/compute/redist/cudnn/v8.7.0/local_installers/11.8/cudnn-linux-x86_64-8.7.0.84_cuda11-archive.tar.xz && \
# tar -xvf ${CUDNN_TAR_FILE} && \
#mv cudnn-linux-x86_64-8.7.0.84_cuda11-archive cuda

### copy the following files into the cuda toolkit directory.
 #mkdir -p /usr/local/cuda/lib64 && \
 #mkdir /usr/local/cuda/include && \
# RUN cp -P cuda/include/cudnn.h /usr/local/cuda/include && \
# cp -P cuda/lib/libcudnn* /usr/local/cuda/lib64/ && \
# chmod a+r /usr/local/cuda/lib64/libcudnn*


# Install Scala kernel
ENV SCALA_VERSION=2.12.8 \
    SCALA_HOME=/usr/share/scala

# NOTE: bash is used by scala/scalac scripts, and it cannot be easily replaced with ash.
RUN apt-get install -y wget ca-certificates && \
    apt-get install -y bash curl jq && \
    cd "/tmp" && \
    wget --no-verbose "https://downloads.typesafe.com/scala/${SCALA_VERSION}/scala-${SCALA_VERSION}.tgz" && \
    tar xzf "scala-${SCALA_VERSION}.tgz" && \
    mkdir "${SCALA_HOME}" && \
    rm "/tmp/scala-${SCALA_VERSION}/bin/"*.bat && \
    mv "/tmp/scala-${SCALA_VERSION}/bin" "/tmp/scala-${SCALA_VERSION}/lib" "${SCALA_HOME}" && \
    ln -s "${SCALA_HOME}/bin/"* "/usr/bin/" && \
    rm -rf "/tmp/"*

#RUN export PATH="/usr/local/sbt/bin:$PATH" && \
#    apk update && apk add ca-certificates wget tar && \
#    mkdir -p "/usr/local/sbt" && \
#    wget -qO - --no-check-certificate "https://cocl.us/sbt-0.13.16.tgz" | \
#    tar xz -C /usr/local/sbt --strip-components=1 && sbt sbtVersion

# Install protoc
RUN PROTOC_VERSION=$(curl -s "https://api.github.com/repos/protocolbuffers/protobuf/releases/latest" | grep -Po '"tag_name": "v\K[0-9.]+') && \
    curl -Lo protoc.zip "https://github.com/protocolbuffers/protobuf/releases/latest/download/protoc-${PROTOC_VERSION}-linux-x86_64.zip" && \
    unzip -q protoc.zip bin/protoc -d /usr/local && \
    chmod a+x /usr/local/bin/protoc

RUN python3 -m pip install --upgrade pip && \
    python3 -m pip install --upgrade build

# Install Hadoop
ENV HADOOP_VERSION=3.3.0 \
    HADOOP_HOME=/usr/local/hadoop \
    HADOOP_OPTS=-Djava.library.path=/usr/local/hadoop/lib/native \
    PATH="$PATH:${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin"

RUN curl https://archive.apache.org/dist/hadoop/core/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}.tar.gz --output hadoop-${HADOOP_VERSION}.tar.gz \
 && tar -zxf hadoop-${HADOOP_VERSION}.tar.gz \
 && rm hadoop-${HADOOP_VERSION}.tar.gz \
 && mv hadoop-${HADOOP_VERSION} ${HADOOP_HOME} \
 && mkdir -p /usr/local/hadoop/logs


# PORTS
# http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.3.0/bk_HDP_Reference_Guide/content/reference_chap2.html
# http://www.cloudera.com/content/cloudera/en/documentation/core/latest/topics/cdh_ig_ports_cdh5.html
# http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/core-default.xml
# http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml

# HDFS: NameNode (NN):
#   9000  = fs.defaultFS			(IPC / File system metadata operations)
#   8022  = dfs.namenode.servicerpc-address	(optional port used by HDFS daemons to avoid sharing RPC port)
#   50070 = dfs.namenode.http-address	(HTTP  / NN Web UI)
#   50470 = dfs.namenode.https-address	(HTTPS / Secure UI)
# HDFS: DataNode (DN):
#   50010 = dfs.datanode.address		(Data transfer)
#   50020 = dfs.datanode.ipc.address	(IPC / metadata operations)
#   50075 = dfs.datanode.http.address	(HTTP  / DN Web UI)
#   50475 = dfs.datanode.https.address	(HTTPS / Secure UI)
# HDFS: Secondary NameNode (SNN)
#   50090 = dfs.secondary.http.address	(HTTP / Checkpoint for NameNode metadata)
EXPOSE 9000 50070 50010 50020 50075 50090

# Install Spark
ENV SPARK_VERSION=3.5.6 \
    SPARK_HOME=/usr/local/spark \
    SPARK_DIST_CLASSPATH="$HADOOP_HOME/etc/hadoop/*:$HADOOP_HOME/share/hadoop/common/lib/*:$HADOOP_HOME/share/hadoop/common/*:$HADOOP_HOME/share/hadoop/hdfs/*:$HADOOP_HOME/share/hadoop/hdfs/lib/*:$HADOOP_HOME/share/hadoop/hdfs/*:$HADOOP_HOME/share/hadoop/yarn/lib/*:$HADOOP_HOME/share/hadoop/yarn/*:$HADOOP_HOME/share/hadoop/mapreduce/lib/*:$HADOOP_HOME/share/hadoop/mapreduce/*:$HADOOP_HOME/share/hadoop/tools/lib/*" \
    PATH="$PATH:${SPARK_HOME}/bin"

RUN curl https://dlcdn.apache.org/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop3.tgz --output spark-${SPARK_VERSION}-bin-hadoop3.tgz  \
 && tar -zxf spark-${SPARK_VERSION}-bin-hadoop3.tgz \
 && rm spark-${SPARK_VERSION}-bin-hadoop3.tgz \
 && mv spark-${SPARK_VERSION}-bin-hadoop3 ${SPARK_HOME}


# "Installing Flink"
ENV FLINK_VERSION=1.12.0 \
    FLINK_HOME=/usr/local/flink \
    PATH="$PATH:${FLINK_HOME}/bin"

#RUN curl https://dlcdn.apache.org/flink/flink-${FLINK_VERSION}/flink-${FLINK_VERSION}-bin-scala_2.12.tgz --output flink-${FLINK_VERSION}-bin-scala_2.12.tgz \
RUN curl https://archive.apache.org/dist/flink/flink-1.12.0/flink-1.12.0-src.tgz --output flink-${FLINK_VERSION}-bin-scala_2.12.tgz \
 && tar -zxf flink-${FLINK_VERSION}-bin-scala_2.12.tgz \
 && rm flink-${FLINK_VERSION}-bin-scala_2.12.tgz \
 && mv flink-${FLINK_VERSION} ${FLINK_HOME}


# "Installing Giraph"
ENV GIRAPH_VERSION=1.3.0 \
    GIRAPH_HOME=/usr/local/giraph \
    PATH="$PATH:${GIRAPH_HOME}/bin"

RUN curl https://archive.apache.org/dist/giraph/giraph-${GIRAPH_VERSION}/giraph-dist-${GIRAPH_VERSION}-hadoop1-bin.tar.gz --output giraph-dist-${GIRAPH_VERSION}-hadoop1-bin.tar.gz \
 && tar -zxf giraph-dist-${GIRAPH_VERSION}-hadoop1-bin.tar.gz \
 && rm giraph-dist-${GIRAPH_VERSION}-hadoop1-bin.tar.gz \
 && mv giraph-${GIRAPH_VERSION}-hadoop1-for-hadoop-1.2.1 ${GIRAPH_HOME}

